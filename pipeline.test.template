// The Informatics Matters Pipeline Test Specification Template.
//
// Copy this file (removing the `.template` extension) and place it next
// to your pipeline script using the same base-name as your script.
// For example, if you have a `experiment_1a.py` pipeline your test file would
// be called `experiment_1a.test`.
//
// This test specification contains lists of tests that define
// excitation parameters for your pipeline and an optional set of
// expectations of what should be seen logged by it and the files it creates.

[

    // The PipelineTester test script version.
    // Must be set and supported by the PipelineTester utility.
    // Check with your administrator and set it to the latest value that is
    // supported.

    version = 1,

    // An optional `setup_collection` section.
    //
    // This defines settings for the entire test suite.
    // At the moment the following settings are supported:
    //
    // timeout: The time allowed for each pipeline test to complete.
    //          Optional. Use this to override the default of 30 seconds.
    //
    // creates: An optional list of file names that each test is expected to
    //          create.  Here we define files created by every test in this
    //          file. Individual tests can define their own test-specific files
    //          with their own 'creates' block, discussed later in this file.
    //
    // There can only be one setup_collection section and it must be
    // the first section in the file.

    setup_collection = [
        timeout: 10,
        creates: [ 'output.txt' ]
    ],

    // Individual tests.
    //
    // Test sections have a name that begins with the word `test`. Add as many
    // as you need in the file in order to fully test the behaviour of your
    // pipeline.
    //
    // Tests consist of blocks of `params` and optional `see` definitions.
    //
    // In the `params` block you must define a parameter value for each option
    // that is defined in your Service Descriptor. The test executor will
    // check that you have done this.
    //
    // The optional `see` block is a list of regular expression strings
    // that are used to inspect the log generated by your pipeline
    // and is used to verify anything significant that is written to the log
    // by your pipeline. For example, if the test is successful when the
    // pipeline logs the text 'Computation = 4.5673' then you would add
    // this to the `see` section (as illustrated below).
    //
    // If a test is not working an you want to keep it and avoid running it
    // then simply prefix the test section name with `ignore_`.
    //
    // The location of any input data you provide in your test is normally
    // located in the project's `data` directory or mounted as `/data`
    // when running in a container. You can safely refer to data in both cases
    // with the PIN environment variable created by the PipelineTester.
    // To allow execution from the command-line and in a container the input
    // data file `blob.dat` should be referred to using `${PIN}blob.dat`

    test_1 = [

        // An optional test-specific 'raw' command.
        //
        // As an alternative to exercising the service-descriptor-supplied
        // command you can specify your own raw command.
        //
        // If you provide a command you **cannot** provide parameters
        // (see the params section below).

        command: ```python my_own_command -i ${PIN}blob.dat
                 --my-own-param-1 32
                 --my-own-param-2 18.5```,

        // Test parameters.
        //
        // Unless you are testing a raw command (see above) you need to
        // provide the built-in service descriptor command wih parameters
        // and values.
        //
        // You can provide values for every option. At the very least
        // you _must_ provide a parameter value for each option that does not
        // have a default.

        params: [ doses: 55,
                  volumes: 0.42,
                  temperature : [minValue: 14.0, maxValue: 22.0] ],

        // Log validation.
        //
        // Optional checks against the pipeline's log.
        // To simplify string checks against output that contains variable
        // whitespace (like tabs etc.) spaces are automatically interpreted
        // as the regular expression '[ \t]+', absorbing any non-line-breaking
        // gaps. You can also use regular expressions.

        see: [ 'Computation = 4.5673' ],

        // Files created (optional).
        //
        // If your pipeline test creates files you can and should declare
        // their names in a `creates` block, either here or in the
        // `setup_collection` section above. All files defined in
        // both the common creates block and the block used here
        // must exist for the test to pass. This test is expected to create
        // and 'output.png' but an 'output.txt' is also expected as that's
        // defined in the 'setup_collection' section (above).

        creates: [ 'output.png' ],

        // Metrics.
        //
        // If your pipeline creates standard metrics - a Java properties file
        // with the name 'output_metrics.txt' - you can use the metrics block
        // here to check its content. You can specify a property
        // (a key) and a regular expression string value.

        metrics: [ __StatusMessage__: 'kel: 0.171, t1/2: 4.06, V: 7.07, CL: 20.1' ]

    ],

    test_2 = [

        params: [ doses: '55,65,75'
                  volumes: 0.82,
                  temperature : [minValue: 14.0, maxValue: 22.0] ],

        see: [ 'Computation = 4.5673',
               'Computation = 5.5673',
               'Computation = 6.5673' ]

    ],

]
